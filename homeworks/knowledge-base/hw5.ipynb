{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/schemas/HW5_Schemas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6gRCRlEyq2s"
      },
      "source": [
        "# HW 5: COMET-ATOMIC Schema\n",
        "\n",
        "In this homework, you will create your own schema to represent the state of a story world as it goes through the story line by line.\n",
        "A **schema** is a structured reprensentation made to hold facts or a plan, which in this case, can be used to track change over time.\n",
        "\n",
        "**The purpose of this homework is to test your understanding of schemas and get hands-on experience with a state-of-the-art tool in commonsense reasoning.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZoWcXimWC4"
      },
      "source": [
        "## Your Task\n",
        "You will be creating a schema using ATOMIC to track the state of a fictional world. For each sentence of the story, you will parse it (provided), call COMET (provided, but what you input is up to you), create preconditions to determine if a sentence can be added (TODO), and create effects to use to update your schema (TODO).\n",
        "\n",
        "Let's teach your agent some basic information about the world!\n",
        "\n",
        "------------------------\n",
        "\n",
        "Formally, the task is:\n",
        "\n",
        "Given an input sentence at time *t* (*In_t*), produce a schema *S_t*. Do this for each sentence in the story.\n",
        "\n",
        "For example, using VerbNet:\n",
        "\n",
        "| Timestep | Input | Schema |\n",
        "|--------|-------|--------|\n",
        "| 1 | Bethany picks up the sword. | `Bethany: has_possesion(sword)` |\n",
        "| 2 | Bethany throws the sword. | `Bethany: !has_possesion(sword)` |\n",
        "\n",
        "*In_1* - \"Bethany picks up the sword.\"\n",
        "\n",
        "would produce\n",
        "\n",
        "*S_1* - `Bethany: has_possesion(sword)`\n",
        "\n",
        "But then if the next sentence is:\n",
        "\n",
        "*In_2* - \"Bethany throws the sword.\"\n",
        "\n",
        "The state would be updated to\n",
        "\n",
        "*S_2* - `Bethany: !has_possesion(sword)`\n",
        "\n",
        "-----------------------------\n",
        "Your resulting system will be sort of like this simplified diagram (the parser is provided for you):\n",
        "![Given a sentence_t and the knowledge representation from your knowledge database of choice, produce schemas (via some schema processor you create)](https://interactive-fiction-class.org/homeworks/schemas/schemas.png)\n",
        "\n",
        "There is some knowledge about the story world, and you are using a schema to feed this information into bite-sized chunks so that your agent (and you) can understand it.\n",
        "You will then have a processing step on the schema where you update it as you get more information as the story progresses.\n",
        "\n",
        "\n",
        "To reiterate, you will:\n",
        "1. Get to know [COMET-ATOMIC-2020](https://aaai.org/ojs/index.php/AAAI/article/view/4160). (Alternate link in case AAAI is down: https://arxiv.org/abs/2010.05953)\n",
        "2. Make a __schema manipulator__ (not a real term, but I think it sounds cool), which will take in knowledge from ATOMIC and spit out your schema. Skeleton code is provided for you, but you are welcome to change things so that it makes more sense to you. This involves two steps:\n",
        "\n",
        "  a. Validate: Generate preconditions to determine if an event can be added.\n",
        "\n",
        "  b. Update: Generate effects to update your world state.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuI9RGzJ94X9"
      },
      "source": [
        "# What is ATOMIC?\n",
        "\n",
        "ATOMIC is a commonsense knowledge graph with some social inferences, among other things.\n",
        "\n",
        "```\n",
        "@inproceedings{sap2019atomic,\n",
        "   title={ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning},\n",
        "   author={Sap, Maarten and LeBras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin},\n",
        "   year={2019},\n",
        "   booktitle={AAAI},\n",
        "   url={https://aaai.org/ojs/index.php/AAAI/article/view/4160}\n",
        "}\n",
        "```\n",
        "\n",
        "It contains the following inferences about people and events:\n",
        "\n",
        "* Because PersonX wanted (xIntent)\n",
        "* Before, PersonX needed (xNeed, HasPrerequisite)\n",
        "* PersonX is seen as (xAttr)\n",
        "* As a result, PersonX feels (xReact)\n",
        "* As a result, PersonX wants (xWant)\n",
        "* As a result, PersonX reasons (xReason)\n",
        "* PersonX then (xEffect)\n",
        "* As a result, others feel (oReact)\n",
        "* As a result, others want (oWant)\n",
        "* Others then (oEffect)\n",
        "* Happens before (isBefore)\n",
        "* Happens after (isAfter)\n",
        "* Is hindered by (HinderedBy)\n",
        "* Causes (Causes)\n",
        "\n",
        "and inferences about entities:\n",
        "* Is located at (AtLocation, LocatedNear, LocationOfAction)\n",
        "* Is made up of (MadeUpOf, PartOf. NotMadeOf)\n",
        "* Is used to (UsedFor, ObjectUse)\n",
        "* Has the property (HasProperty, NotHasProperty)\n",
        "* Is capable of (CapableOf, NotCapableOf)\n",
        "* Desires (Desires, NotDesires)\n",
        "\n",
        "\n",
        "Among other things (full list is in the `all_relations` variable below).\n",
        "\n",
        "\n",
        "[COMET](https://aclanthology.org/P19-1470) is the model that is trained on ATOMIC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC8ocsUWWill"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1LjnvrV-LbA"
      },
      "source": [
        "## Get COMET and Install packages\n",
        "\n",
        "Repo: https://github.com/allenai/comet-atomic-2020/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UPPEQeYigKP8"
      },
      "outputs": [],
      "source": [
        "# Clone the repo\n",
        "%%capture\n",
        "!git clone https://github.com/allenai/comet-atomic-2020.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Modr9o4NWRsO"
      },
      "outputs": [],
      "source": [
        "# Enter the directory\n",
        "import os\n",
        "os.chdir('comet-atomic-2020')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I9-JvoaOTwvB"
      },
      "outputs": [],
      "source": [
        "# Install ATOMIC's dependencies\n",
        "%%capture\n",
        "!pip install rouge_score\n",
        "!pip install transformers\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukxhQxiooue2"
      },
      "source": [
        "### Also, Stanford's Stanza Parser\n",
        "\n",
        "In order to process the input sentences, you will need to do some parsing. Here, we have setup Stanford's English language NER (Named Entity Recognition) and constituency parser using [Stanza](https://stanfordnlp.github.io/stanza/index.html). You're also welcome to use any of the other parsers they provide (or even switch to a completely different type of parser that you like better)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "x7KgiGoPKnQt",
        "outputId": "2ac3fb7f-88fd-46f0-dd44-6b7c27588ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: en (English) ...\n",
            "INFO:stanza:Downloaded file to /root/stanza_resources/en/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!python -m pip install stanza\n",
        "\n",
        "import stanza\n",
        "stanza.download('en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaIyiwTv9kFA"
      },
      "source": [
        "**You might need to restart the runtime now.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dee09bcc6ee14d5f9a505571d8f1ce60",
            "21781cc9e0d9445ea70f84bd43a27bda",
            "38538c5b58604b7c9e7729f327e15536",
            "125b4edeb1e846beb6669431c5cb5b4f",
            "b81c053311104e8b826fe513ef16da58",
            "ef24fc0e50f74952927837dc08a54750",
            "940dae3f76da45b0b68ffa75fe267d0c",
            "d7a214e6f26f47f6b172069722ad9957",
            "52b55874b5bf44fd9068609112db62f8",
            "cb3b8dc4ff6745568bdeb44868f90b4a",
            "8dfccf4a127a410a832b5d082c360c59"
          ]
        },
        "id": "j_IpCvvOlYjH",
        "outputId": "2800bb5f-dbde-4e92-8efd-5aa2d8a6fbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dee09bcc6ee14d5f9a505571d8f1ce60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "=========================================\n",
            "| Processor | Package                   |\n",
            "-----------------------------------------\n",
            "| tokenize  | combined                  |\n",
            "| mwt       | combined                  |\n",
            "| ner       | ontonotes-ww-multi_charlm |\n",
            "=========================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Loading: mwt\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Loading: ner\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'id': 1,\n",
              "   'text': 'Aurora',\n",
              "   'start_char': 0,\n",
              "   'end_char': 6,\n",
              "   'ner': 'S-PERSON',\n",
              "   'multi_ner': ['S-PERSON']},\n",
              "  {'id': 2,\n",
              "   'text': 'submitted',\n",
              "   'start_char': 7,\n",
              "   'end_char': 16,\n",
              "   'ner': 'O',\n",
              "   'multi_ner': ['O']},\n",
              "  {'id': 3,\n",
              "   'text': 'her',\n",
              "   'start_char': 17,\n",
              "   'end_char': 20,\n",
              "   'ner': 'O',\n",
              "   'multi_ner': ['O']},\n",
              "  {'id': 4,\n",
              "   'text': 'resignation',\n",
              "   'start_char': 21,\n",
              "   'end_char': 32,\n",
              "   'ner': 'O',\n",
              "   'multi_ner': ['O']},\n",
              "  {'id': 5,\n",
              "   'text': 'to',\n",
              "   'start_char': 33,\n",
              "   'end_char': 35,\n",
              "   'ner': 'O',\n",
              "   'multi_ner': ['O']},\n",
              "  {'id': 6,\n",
              "   'text': 'Facebook',\n",
              "   'start_char': 36,\n",
              "   'end_char': 44,\n",
              "   'ner': 'S-ORG',\n",
              "   'multi_ner': ['S-ORG'],\n",
              "   'misc': 'SpaceAfter=No'},\n",
              "  {'id': 7,\n",
              "   'text': '.',\n",
              "   'start_char': 44,\n",
              "   'end_char': 45,\n",
              "   'ner': 'O',\n",
              "   'multi_ner': ['O'],\n",
              "   'misc': 'SpaceAfter=No'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Example code to run Stanza\n",
        "\n",
        "import stanza #here's the re-import for when your runtime is restarted\n",
        "import json\n",
        "nlp = stanza.Pipeline('en', processors='tokenize, ner, mwt') #lemma, depparse, constituency, pos\n",
        "parse = nlp(\"Aurora submitted her resignation to Facebook.\")\n",
        "y = json.loads(str(parse))\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9LWpLE1VZ_F"
      },
      "source": [
        "## COMET-ATOMIC-2020 (BART) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0PMpNE3zch4z"
      },
      "outputs": [],
      "source": [
        "# Download the model\n",
        "%%capture\n",
        "!bash models/comet_atomic2020_bart/download_model.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHxOhhBYelSf"
      },
      "source": [
        "**Tip: Take note of the `all_relations` dictionary below! You will need it later on.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQmrCl-pcSKf",
        "outputId": "10434fa7-17e4-4567-d30c-0da8fe4a3f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loading ...\n",
            "model loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "# copied from models/comet_atomic2020_bart/generation_example.py\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from models.comet_atomic2020_bart.utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
        "\n",
        "\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "\n",
        "class Comet:\n",
        "    def __init__(self, model_path):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        task = \"summarization\"\n",
        "        use_task_specific_params(self.model, task)\n",
        "        self.batch_size = 1\n",
        "        self.decoder_start_token_id = None\n",
        "\n",
        "    def generate(\n",
        "            self,\n",
        "            queries,\n",
        "            decode_method=\"beam\",\n",
        "            num_generate=5,\n",
        "            ):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            examples = queries\n",
        "\n",
        "            decs = []\n",
        "            for batch in list(chunks(examples, self.batch_size)):\n",
        "\n",
        "                batch = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
        "                input_ids, attention_mask = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
        "\n",
        "                summaries = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_start_token_id=self.decoder_start_token_id,\n",
        "                    num_beams=num_generate,\n",
        "                    num_return_sequences=num_generate,\n",
        "                    )\n",
        "\n",
        "                dec = self.tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "                decs.append(dec)\n",
        "\n",
        "            return decs\n",
        "\n",
        "\n",
        "all_relations = [\n",
        "    \"AtLocation\",\n",
        "    \"CapableOf\",\n",
        "    \"Causes\",\n",
        "    \"CausesDesire\",\n",
        "    \"CreatedBy\",\n",
        "    \"DefinedAs\",\n",
        "    \"DesireOf\",\n",
        "    \"Desires\",\n",
        "    \"HasA\",\n",
        "    \"HasFirstSubevent\",\n",
        "    \"HasLastSubevent\",\n",
        "    \"HasPainCharacter\",\n",
        "    \"HasPainIntensity\",\n",
        "    \"HasPrerequisite\",\n",
        "    \"HasProperty\",\n",
        "    \"HasSubEvent\",\n",
        "    \"HasSubevent\",\n",
        "    \"HinderedBy\",\n",
        "    \"InheritsFrom\",\n",
        "    \"InstanceOf\",\n",
        "    \"IsA\",\n",
        "    \"LocatedNear\",\n",
        "    \"LocationOfAction\",\n",
        "    \"MadeOf\",\n",
        "    \"MadeUpOf\",\n",
        "    \"MotivatedByGoal\",\n",
        "    \"NotCapableOf\",\n",
        "    \"NotDesires\",\n",
        "    \"NotHasA\",\n",
        "    \"NotHasProperty\",\n",
        "    \"NotIsA\",\n",
        "    \"NotMadeOf\",\n",
        "    \"ObjectUse\",\n",
        "    \"PartOf\",\n",
        "    \"ReceivesAction\",\n",
        "    \"RelatedTo\",\n",
        "    \"SymbolOf\",\n",
        "    \"UsedFor\",\n",
        "    \"isAfter\",\n",
        "    \"isBefore\",\n",
        "    \"isFilledBy\",\n",
        "    \"oEffect\",\n",
        "    \"oReact\",\n",
        "    \"oWant\",\n",
        "    \"xAttr\",\n",
        "    \"xEffect\",\n",
        "    \"xIntent\",\n",
        "    \"xNeed\",\n",
        "    \"xReact\",\n",
        "    \"xReason\",\n",
        "    \"xWant\",\n",
        "    ]\n",
        "\n",
        "print(\"model loading ...\")\n",
        "comet = Comet(\"./comet-atomic_2020_BART\")\n",
        "comet.model.zero_grad()\n",
        "print(\"model loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg8SSJmhem8Z"
      },
      "source": [
        "Run your queries. `head` is the input sentence, and `rel` is the relation, as seen in `all_relations`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UukXJgBmdskV",
        "outputId": "7f4b4803-945e-485d-9084-24c81734dc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PersonX relies on PersonY xNeed [GEN]']\n",
            "[[' to know PersonY', ' to be dependent on someone', ' to ask for help', ' to be dependent', ' none']]\n"
          ]
        }
      ],
      "source": [
        "# Example code to run COMET. A method has been written for you further down called callCOMET().\n",
        "queries = []\n",
        "head = \"PersonX relies on PersonY\"\n",
        "rel = \"xNeed\"\n",
        "query = \"{} {} [GEN]\".format(head, rel)\n",
        "queries.append(query)\n",
        "print(queries)\n",
        "results = comet.generate(queries, decode_method=\"beam\", num_generate=5)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y-PuV-j5d8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f9ccd7f-8b2e-466e-f1dd-e21406bddf40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Optional: Get ATOMIC data (if you wanted the train/test/val sets)\\n!wget https://ai2-atomic.s3-us-west-2.amazonaws.com/data/atomic2020_data-feb2021.zip\\n!unzip atomic2020_data-feb2021.zip\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Optional: Get ATOMIC data (if you wanted the train/test/val sets)\n",
        "!wget https://ai2-atomic.s3-us-west-2.amazonaws.com/data/atomic2020_data-feb2021.zip\n",
        "!unzip atomic2020_data-feb2021.zip\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJDTUKQyjHL4"
      },
      "source": [
        "# Parse the sentence to feed into COMET\n",
        "You're welcome to change this to fit your needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4nK_609jTi_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "class SentParser:\n",
        "  \"\"\"\n",
        "  Parse the sentence and get the entities and the new phrase with tags\n",
        "  \"\"\"\n",
        "  def __init__(self, sentence):\n",
        "    sentence = sentence.replace(\".\",\"\")\n",
        "    parse = nlp(sentence) # call Stanza\n",
        "    self.phrase = sentence # original sentence\n",
        "    self.entities = dict() # dict of labels for entities e.g., [{PersonX: John}]\n",
        "    self.new_phrase = sentence # the sentence with person names changed to PersonX and PersonY\n",
        "\n",
        "    for sentence in parse.sentences:\n",
        "      ents = self.getEntities(sentence.tokens)\n",
        "      self.entities = ents\n",
        "\n",
        "      for tag in ents.keys():\n",
        "        person = ents[tag]\n",
        "        self.new_phrase = self.new_phrase.replace(person, tag)\n",
        "\n",
        "  def getEntities(self, parse):\n",
        "    \"\"\"\n",
        "    get the named entities so you can pass it to ATOMIC's input and\n",
        "    fill the PersonX and PersonY tags from the output\n",
        "\n",
        "    args:\n",
        "    parse (list) - list of Stanza token objects for this phrase\n",
        "\n",
        "    return:\n",
        "    entities (dict) - keeps track of who is PersonX and PersonY e.g. {PersonX: John}\n",
        "    \"\"\"\n",
        "    entities = dict()\n",
        "    count = 0\n",
        "    for word in parse:\n",
        "      if \"PERSON\" in word.ner:\n",
        "        if count == 0:\n",
        "          entities['PersonX'] = word.text\n",
        "          count+=1\n",
        "        elif count == 1:\n",
        "          entities['PersonY'] = word.text\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U_9dFk0mmcn"
      },
      "outputs": [],
      "source": [
        "# Example call\n",
        "s = SentParser(\"John went to the bank.\")\n",
        "print(s.phrase)\n",
        "print(s.entities)\n",
        "print(s.new_phrase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYd9fKe_im9A"
      },
      "source": [
        "# TODO: Setup your schema\n",
        "\n",
        "Use a subset of the relations from `all_relations` for what should be a pre-condition and what should be an effect (or both!). Work with whatever you think makes sense.\n",
        "\n",
        "Tip: You might want to not take every single fact that ATOMIC gives you. Try to come up with a heuristic to just take what you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbqcULLzjC9_"
      },
      "outputs": [],
      "source": [
        "def callCOMET(sent, rel, decode=\"beam\", num=5):\n",
        "  \"\"\"\n",
        "  Making COMET generate facts based on an input sent and a relation rel\n",
        "  You can also provide the decoding method and the number of facts\n",
        "  you want it to output.\n",
        "  \"\"\"\n",
        "  query = [\"{} {} [GEN]\".format(sent, rel)]\n",
        "  gen = comet.generate(query, decode_method=decode, num_generate=num)[0]\n",
        "  return [s.strip().replace(\".\",\"\") for s in gen if s.strip() != \"none\" or s.strip() != \".\"]\n",
        "\n",
        "def fillEntityTags(fact, NER):\n",
        "  \"\"\"\n",
        "  Given a output fact from COMET (str) and the NER (dict), replace the PersonX/Y\n",
        "  tags with their original names\n",
        "  \"\"\"\n",
        "  new = fact\n",
        "  for entity in NER.keys():\n",
        "    new = new.replace(entity,NER[entity])\n",
        "  return new\n",
        "\n",
        "class Predicate:\n",
        "  \"\"\"\n",
        "  Individual precondition and effect objects\n",
        "  \"\"\"\n",
        "  def __init__(self, rel, statement, isPre, neg=False):\n",
        "    self.relation = rel # string - input COMET relation\n",
        "    self.statement = statement # string - results from COMET\n",
        "    self.isPrecondition = isPre # boolean - if it's a precondition (True) or an effect (False)\n",
        "    self.isNegated = neg # boolean - if it's negated (True) or not (False)\n",
        "\n",
        "class Schema:\n",
        "  \"\"\"\n",
        "  Your schema\n",
        "  \"\"\"\n",
        "  def __init__(self, starting_state):\n",
        "    self.state = starting_state # defaultdict(set) - a set of facts for each entity\n",
        "    self.curr_event = \"\" # string - received from the parser; phrase from the original input sentence with PersonX/PersonY labels\n",
        "    self.curr_NER = dict() # dict - person names and their corresponding tags for a given subevent/phrase\n",
        "    self.preconditions = [] # list - stores Predicate objects\n",
        "    self.timestep = 0 # int - how far you are in the story (optional)\n",
        "\n",
        "  ### Check the preconditions against the state ###\n",
        "  def checkPrecondition(self, pred):\n",
        "    \"\"\"\n",
        "    Given this precondition and the current state, does this precondition pass?\n",
        "    args:\n",
        "    pred (Predicate)\n",
        "\n",
        "    return:\n",
        "    boolean - whether or not this event is valid\n",
        "    \"\"\"\n",
        "    if pred.isPrecondition == False: return False #it's an effect, don't consider it\n",
        "\n",
        "    ### Your code here ###\n",
        "    #TODO: check pred against self.state\n",
        "    # You can do direct matches or \"close enough\" (i.e., similarity) matches\n",
        "    #e.g.:\n",
        "    if pred.statement not in self.state[self.curr_NER['PersonX']]:\n",
        "      return False\n",
        "    ######################\n",
        "\n",
        "    return True\n",
        "\n",
        "  def getPreconditions(self):\n",
        "    \"\"\"\n",
        "    Given the input event string (self.curr_event),\n",
        "    return a list of preconditions (list of Predicate objects)\n",
        "    \"\"\"\n",
        "    pre = []\n",
        "\n",
        "    ### Your code here ###\n",
        "    #TODO: complete the list using the **relevant** relations from \"all_relations\"\n",
        "    #an example:\n",
        "    rel = \"HasPrerequisite\"\n",
        "    comet_out = callCOMET(self.curr_event,rel)\n",
        "    for fact in comet_out:\n",
        "      filled = fillEntityTags(fact,self.curr_NER)\n",
        "      pre.append(Predicate(rel, filled, True)) #Predicate(rel, statement, isPrecondition, negated)\n",
        "    ######################\n",
        "\n",
        "    return pre\n",
        "\n",
        "  def checkValidity(self):\n",
        "    \"\"\"\n",
        "    Goes through all the preconditions to check to see if\n",
        "    this event can be added to the state.\n",
        "    A precondition is considered valid as long as\n",
        "\n",
        "    return:\n",
        "    boolean - whether or not this event is valid\n",
        "    \"\"\"\n",
        "    print(\"State:\",self.state)\n",
        "    preconds = self.getPreconditions()\n",
        "    valids = []\n",
        "    for precond in preconds:\n",
        "      print(precond.statement)\n",
        "      valid = self.checkPrecondition(precond)\n",
        "      print(\"Valid\",valid)\n",
        "      if valid:\n",
        "        valids.append(precond)\n",
        "    if valids:\n",
        "      self.preconditions += valids\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "\n",
        "  ### Once validated, update the schema ###\n",
        "  def getEffects(self):\n",
        "    \"\"\"\n",
        "    Given the input event string (self.curr_event),\n",
        "    return a list of effects (list of Predicate objects)\n",
        "    \"\"\"\n",
        "    effects = []\n",
        "\n",
        "    ### Your code here ###\n",
        "    #TODO: complete the list using the **relevant** relations from \"all_relations\"\n",
        "    #an example:\n",
        "    rel = \"xReact\"\n",
        "    fact = callCOMET(self.curr_event,rel)[0]\n",
        "    filled = fillEntityTags(fact,self.curr_NER)\n",
        "    effects.append(Predicate(rel, filled, False)) #Predicate(rel, statement, isPrecondition, negated)\n",
        "    ######################\n",
        "\n",
        "    return effects\n",
        "\n",
        "\n",
        "  def updateSchema(self, event, NER_dict):\n",
        "    \"\"\"\n",
        "    Given an input event string (event), check the validity of adding it to the state,\n",
        "    and update the schema state (self.state) with new effects\n",
        "    args:\n",
        "    event(str) - received from the parser; phrase from the original input sentence with PersonX/PersonY labels\n",
        "    NER_dict(dict) - person names and their corresponding tags for a given subevent/phrase, e.g. NER_dict['PersonX'] = \"Cindy\"\n",
        "    \"\"\"\n",
        "    self.curr_event = event\n",
        "    self.curr_NER = NER_dict\n",
        "\n",
        "    #1) check validity\n",
        "    valid = self.checkValidity()\n",
        "    print(\"Preconditions:\",self.preconditions[0].statement)\n",
        "\n",
        "    #2) update the state\n",
        "    effects = self.getEffects()\n",
        "    print(\"Effects:\", effects[0].statement)\n",
        "\n",
        "\n",
        "    ### Your code here ###\n",
        "\n",
        "    #TODO: add these new effects to the state\n",
        "    for effect in effects:\n",
        "      if effect.relation == \"xReact\":\n",
        "        self.state[NER_dict['PersonX']].add(effect.statement)\n",
        "\n",
        "    # TODO: (extra credit) remove facts that aren't true anymore\n",
        "    # Although this is ideal, it might be hard to figure out when facts are negated\n",
        "\n",
        "    ######################\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEbMOVcneM0X"
      },
      "source": [
        "# What to Turn In\n",
        "* Your code\n",
        "* A brief description/diagram of the structure of your schema\n",
        "* Your answers to these two sets of questions:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V31TrjW2UDJW"
      },
      "source": [
        "## Story Tracking Questions\n",
        "Run the following stories through your system and print out your schema after each sentence (or subevent if there are multiple events in a sentence).\n",
        "For each scenario, **keep the print out of your schema for that story in your ipynb**.\n",
        "\n",
        "**Do not change your schema code in between running these examples! Your final schema code should be able to run multiple scenarios.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkN6CxQ8PNbq"
      },
      "outputs": [],
      "source": [
        "# Function for updating the schema throughout a story\n",
        "def runStory(story, start = defaultdict(set)):\n",
        "  schema = Schema(start)\n",
        "  for sent in story:\n",
        "    print(sent)\n",
        "    s = SentParser(sent)\n",
        "    try:\n",
        "      schema.updateSchema(s.new_phrase,s.entities)\n",
        "      print(\"Schema:\",schema.state)\n",
        "    except:\n",
        "      print(\"Story fails!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing call 1\n",
        "start = defaultdict(set)\n",
        "start.update({\"John\": set([\"John is hungry.\"])})\n",
        "runStory([\"John eats an apple.\"], start)"
      ],
      "metadata": {
        "id": "f-uuZsG57fKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing call 2\n",
        "start = defaultdict(set)\n",
        "start.update({\"Imani\": set([\"Imani has a toothache.\"])})\n",
        "runStory([\"Imani made an appointment to see the dentist.\", \"The dentist told Imani that she had a cavity.\", \"Imani never had a cavity before.\", \"Imani was not looking forward to her next dentist appointment.\"], start)"
      ],
      "metadata": {
        "id": "QmM4Kd0q7g_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please keep the following code blocks commented out until you finish your schema."
      ],
      "metadata": {
        "id": "KWTKVv2H7QuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fjIg_wvKMNjQ"
      },
      "outputs": [],
      "source": [
        "stories = {\n",
        "    1: [\"Gina misplaced her phone.\", \"Gina looks for her phone in the living room.\", \"Gina remembers leaving her phone in the car.\", \"Gina goes back to the car.\", \"Gina finds her phone in the car.\"],\n",
        "    2: [\"Phil was at the community pool.\",\"Phil thought he could go out to the deeper end by himself.\",\"Phil jumps into the deep end.\",\"Phil has trouble staying afloat.\",\"The lifeguard had to help Phil out of the water.\"],\n",
        "    3: [\"Amy was happy her first class in junior high was all new kids.\", \"Amy introduced herself to the girl seated next to her.\", \"The girl was even more nervous than Amy to make friends.\", \"The girls talked and bonded over their love of books.\", \"The girls decided to meet up after school to go to the library.\"],\n",
        "    4: [\"Xander's dog hates his treats.\", \"Xander decided to go buy some new dog treats.\", \"None of the dog treats at the pet store looked tasty.\", \"Xander decided to buy his dog some salmon from the fish market.\", \"Xander's dog loved the salmon.\"],\n",
        "    5: [\"Franco has never cooked for his family.\", \"Franco decided to follow an old family recipe.\", \"Franco's grandma told him anybody could make the recipe.\", \"Franco made a whole meal for his family in one hour.\", \"Franco's family all loved the meal.\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Story 1\n",
        "\"\"\"\n",
        "start = defaultdict(set)\n",
        "start['Gina'] = set([stories[1][0]])\n",
        "runStory(stories[1][1:], start)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HTAKQi6JiZuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Story 2\n",
        "\"\"\"\n",
        "start = defaultdict(set)\n",
        "start['Phil'] = set([stories[2][0]])\n",
        "runStory(stories[2][1:], start)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lgRNOpVxxvZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Story 3\n",
        "\"\"\"\n",
        "start = defaultdict(set)\n",
        "start['Amy'] = set([stories[3][0]])\n",
        "runStory(stories[3][1:], start)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_ZSLr6kHxwsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Story 4\n",
        "\"\"\"\n",
        "start = defaultdict(set)\n",
        "start['Xander'] = set([stories[4][0]])\n",
        "runStory(stories[4][1:], start)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S20-miBBxwMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Story 5\n",
        "\"\"\"\n",
        "start = defaultdict(set)\n",
        "start['Franco'] = set([stories[5][0]])\n",
        "runStory(stories[5][1:], start)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8VWrx3YLxv37"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PC8ocsUWWill"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dee09bcc6ee14d5f9a505571d8f1ce60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21781cc9e0d9445ea70f84bd43a27bda",
              "IPY_MODEL_38538c5b58604b7c9e7729f327e15536",
              "IPY_MODEL_125b4edeb1e846beb6669431c5cb5b4f"
            ],
            "layout": "IPY_MODEL_b81c053311104e8b826fe513ef16da58"
          }
        },
        "21781cc9e0d9445ea70f84bd43a27bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef24fc0e50f74952927837dc08a54750",
            "placeholder": "​",
            "style": "IPY_MODEL_940dae3f76da45b0b68ffa75fe267d0c",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: "
          }
        },
        "38538c5b58604b7c9e7729f327e15536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7a214e6f26f47f6b172069722ad9957",
            "max": 48453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52b55874b5bf44fd9068609112db62f8",
            "value": 48453
          }
        },
        "125b4edeb1e846beb6669431c5cb5b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3b8dc4ff6745568bdeb44868f90b4a",
            "placeholder": "​",
            "style": "IPY_MODEL_8dfccf4a127a410a832b5d082c360c59",
            "value": " 392k/? [00:00&lt;00:00, 17.1MB/s]"
          }
        },
        "b81c053311104e8b826fe513ef16da58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef24fc0e50f74952927837dc08a54750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "940dae3f76da45b0b68ffa75fe267d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7a214e6f26f47f6b172069722ad9957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52b55874b5bf44fd9068609112db62f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb3b8dc4ff6745568bdeb44868f90b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dfccf4a127a410a832b5d082c360c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}